{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:17.953615Z",
     "iopub.status.busy": "2024-11-05T04:58:17.952915Z",
     "iopub.status.idle": "2024-11-05T04:58:17.962711Z",
     "shell.execute_reply": "2024-11-05T04:58:17.961348Z",
     "shell.execute_reply.started": "2024-11-05T04:58:17.953574Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = './input_df' if os.path.exists('./input_df') else '/kaggle/input/js24-preprocessing-create-lags/training.parquet'\n",
    "TRAINING = True\n",
    "feature_names = [f\"feature_{i:02d}\" for i in range(79)] + [f\"responder_{idx}_lag_1\" for idx in range(9)]\n",
    "label_name = 'responder_6'\n",
    "weight_name = 'weight'\n",
    "train_name = os.path.join(\"./input_df/\", \"nn_input_df_with_lags.pickle\")\n",
    "valid_name = os.path.join(\"./input_df/\", \"nn_valid_df_with_lags.pickle\")\n",
    "if TRAINING and not os.path.exists(train_name):\n",
    "    df = pl.scan_parquet(f\"{input_path}/training.parquet\").collect().to_pandas()\n",
    "    valid = pl.scan_parquet(f\"{input_path}/validation.parquet\").collect().to_pandas()\n",
    "    df = pd.concat([df, valid]).reset_index(drop=True)# A trick to boost LB from 0.0045->0.005\n",
    "    with open(train_name, \"wb\") as w:\n",
    "        pickle.dump(df, w)\n",
    "    with open(valid_name, \"wb\") as w:\n",
    "        pickle.dump(valid, w)\n",
    "elif TRAINING:\n",
    "    with open(train_name, \"rb\") as r:\n",
    "        df = pickle.load(r)\n",
    "    with open(valid_name, \"rb\") as r:\n",
    "        valid = pickle.load(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[ feature_names ]\n",
    "y_train = df[ label_name ]\n",
    "w_train = df[ \"weight\" ]\n",
    "X_valid = valid[ feature_names ]\n",
    "y_valid = valid[ label_name ]\n",
    "w_valid = valid[ \"weight\" ]\n",
    "\n",
    "X_train.shape, y_train.shape, w_train.shape, X_valid.shape, y_valid.shape, w_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:17.965474Z",
     "iopub.status.busy": "2024-11-05T04:58:17.964754Z",
     "iopub.status.idle": "2024-11-05T04:58:17.993759Z",
     "shell.execute_reply": "2024-11-05T04:58:17.992851Z",
     "shell.execute_reply.started": "2024-11-05T04:58:17.965427Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import (LightningDataModule, LightningModule, Trainer)\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Timer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class custom_args():\n",
    "    def __init__(self):\n",
    "        self.usegpu = True\n",
    "        self.gpuid = 0\n",
    "        self.seed = 42\n",
    "        self.model = 'nn'\n",
    "        self.use_wandb = False\n",
    "        self.project = 'js-xs-nn-with-lags'\n",
    "        self.dname = \"./input_df/\"\n",
    "        self.loader_workers = 4\n",
    "        self.bs = 8192\n",
    "        self.lr = 1e-3\n",
    "        self.weight_decay = 5e-4\n",
    "        self.dropouts = [0.1, 0.1]\n",
    "        self.n_hidden = [512, 512, 256]\n",
    "        self.patience = 25\n",
    "        self.max_epochs = 2000\n",
    "        self.N_fold = 5\n",
    "\n",
    "\n",
    "my_args = custom_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:17.995267Z",
     "iopub.status.busy": "2024-11-05T04:58:17.994912Z",
     "iopub.status.idle": "2024-11-05T04:58:18.776454Z",
     "shell.execute_reply": "2024-11-05T04:58:18.77564Z",
     "shell.execute_reply.started": "2024-11-05T04:58:17.995223Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, accelerator):\n",
    "        self.features = torch.FloatTensor(df[feature_names].values).to(accelerator)\n",
    "        self.labels = torch.FloatTensor(df[label_name].values).to(accelerator)\n",
    "        self.weights = torch.FloatTensor(df[weight_name].values).to(accelerator)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        y = self.labels[idx]\n",
    "        w = self.weights[idx]\n",
    "        return x, y, w\n",
    "\n",
    "\n",
    "class DataModule(LightningDataModule):\n",
    "    def __init__(self, train_df, batch_size, valid_df=None, accelerator='cpu'):\n",
    "        super().__init__()\n",
    "        self.df = train_df\n",
    "        self.batch_size = batch_size\n",
    "        self.dates = self.df['date_id'].unique()\n",
    "        self.accelerator = accelerator\n",
    "        self.train_dataset = None\n",
    "        self.valid_df = None\n",
    "        if valid_df is not None:\n",
    "            self.valid_df = valid_df\n",
    "        self.val_dataset = None\n",
    "\n",
    "    def setup(self, fold=0, N_fold=5, stage=None):\n",
    "        # Split dataset\n",
    "        selected_dates = [date for ii, date in enumerate(self.dates) if ii % N_fold != fold]\n",
    "        df_train = self.df.loc[self.df['date_id'].isin(selected_dates)]\n",
    "        self.train_dataset = CustomDataset(df_train, self.accelerator)\n",
    "        if self.valid_df is not None:\n",
    "            df_valid = self.valid_df\n",
    "            self.val_dataset = CustomDataset(df_valid, self.accelerator)\n",
    "\n",
    "    def train_dataloader(self, n_workers=0):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=n_workers)\n",
    "\n",
    "    def val_dataloader(self, n_workers=0):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=n_workers)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:18.778953Z",
     "iopub.status.busy": "2024-11-05T04:58:18.778669Z",
     "iopub.status.idle": "2024-11-05T04:58:18.796672Z",
     "shell.execute_reply": "2024-11-05T04:58:18.795818Z",
     "shell.execute_reply.started": "2024-11-05T04:58:18.778923Z"
    }
   },
   "outputs": [],
   "source": [
    "# Custom R2 metric for validation\n",
    "def r2_val(y_true, y_pred, sample_weight):\n",
    "    r2 = 1 - np.average((y_pred - y_true) ** 2, weights=sample_weight) / (np.average((y_true) ** 2, weights=sample_weight) + 1e-38)\n",
    "    return r2\n",
    "\n",
    "\n",
    "class NN(LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dims, dropouts, lr, weight_decay):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        layers = []\n",
    "        in_dim = input_dim\n",
    "        for i, hidden_dim in enumerate(hidden_dims):\n",
    "            layers.append(nn.BatchNorm1d(in_dim))\n",
    "            if i > 0:\n",
    "                layers.append(nn.SiLU())\n",
    "            if i < len(dropouts):\n",
    "                layers.append(nn.Dropout(dropouts[i]))\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            # layers.append(nn.ReLU())\n",
    "            in_dim = hidden_dim\n",
    "        layers.append(nn.Linear(in_dim, 1)) \n",
    "        layers.append(nn.Tanh())\n",
    "        self.model = nn.Sequential(*layers)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.validation_step_outputs = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 5 * self.model(x).squeeze(-1)  \n",
    "\n",
    "    def training_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w  #\n",
    "        loss = loss.mean()\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch):\n",
    "        x, y, w = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.mse_loss(y_hat, y, reduction='none') * w\n",
    "        loss = loss.mean()\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "        self.validation_step_outputs.append((y_hat, y, w))\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Calculate validation WRMSE at the end of the epoch.\"\"\"\n",
    "        y = torch.cat([x[1] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        if self.trainer.sanity_checking:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "        else:\n",
    "            prob = torch.cat([x[0] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            weights = torch.cat([x[2] for x in self.validation_step_outputs]).cpu().numpy()\n",
    "            # r2_val\n",
    "            val_r_square = r2_val(y, prob, weights)\n",
    "            self.log(\"val_r_square\", val_r_square, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5,\n",
    "                                                               verbose=True)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_loss',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        if self.trainer.sanity_checking:\n",
    "            return\n",
    "        epoch = self.trainer.current_epoch\n",
    "        metrics = {k: v.item() if isinstance(v, torch.Tensor) else v for k, v in self.trainer.logged_metrics.items()}\n",
    "        formatted_metrics = {k: f\"{v:.5f}\" for k, v in metrics.items()}\n",
    "        print(f\"Epoch {epoch}: {formatted_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = my_args\n",
    "\n",
    "# checking device\n",
    "device = torch.device(f'cuda:{args.gpuid}' if torch.cuda.is_available() and args.usegpu else 'cpu')\n",
    "accelerator = 'gpu' if torch.cuda.is_available() and args.usegpu else 'cpu'\n",
    "loader_device = 'cpu'\n",
    "\n",
    "\n",
    "# Initialize Data Module\n",
    "\n",
    "df[feature_names] = df[feature_names].fillna(method = 'ffill').fillna(0)\n",
    "valid[feature_names] = valid[feature_names].fillna(method = 'ffill').fillna(0)\n",
    "data_module = DataModule(df, batch_size=args.bs, valid_df=valid, accelerator=loader_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-05T04:58:18.798262Z",
     "iopub.status.busy": "2024-11-05T04:58:18.797992Z",
     "iopub.status.idle": "2024-11-05T04:58:18.818565Z",
     "shell.execute_reply": "2024-11-05T04:58:18.817654Z",
     "shell.execute_reply.started": "2024-11-05T04:58:18.798232Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "del df\n",
    "gc.collect()\n",
    "pl.seed_everything(args.seed)\n",
    "for fold in range(args.N_fold):\n",
    "    data_module.setup(fold, args.N_fold)\n",
    "    # Obtain input dimension\n",
    "    input_dim = data_module.train_dataset.features.shape[1]\n",
    "    # Initialize Model\n",
    "    model = NN(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dims=args.n_hidden,\n",
    "        dropouts=args.dropouts,\n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "    # Initialize Logger\n",
    "    if args.use_wandb:\n",
    "        wandb_run = wandb.init(project=args.project, config=vars(args), reinit=True)\n",
    "        logger = WandbLogger(experiment=wandb_run)\n",
    "    else:\n",
    "        logger = None\n",
    "    # Initialize Callbacks\n",
    "    early_stopping = EarlyStopping('val_loss', patience=args.patience, mode='min', verbose=False)\n",
    "    checkpoint_callback = ModelCheckpoint(monitor='val_loss', mode='min', save_top_k=1, verbose=False, filename=f\"./models/nn_{fold}.model\") \n",
    "    timer = Timer()\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        max_epochs=args.max_epochs,\n",
    "        accelerator=accelerator,\n",
    "        devices=[args.gpuid] if args.usegpu else None,\n",
    "        logger=logger,\n",
    "        callbacks=[early_stopping, checkpoint_callback, timer],\n",
    "        enable_progress_bar=True\n",
    "    )\n",
    "    # Start Training\n",
    "    trainer.fit(model, data_module.train_dataloader(args.loader_workers), data_module.val_dataloader(args.loader_workers))\n",
    "    # You can find trained best model in your local path\n",
    "    print(f'Fold-{fold} Training completed in {timer.time_elapsed(\"train\"):.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "sourceId": 203900450,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.594014,
   "end_time": "2024-10-10T11:58:36.355301",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-10T11:58:28.761287",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
